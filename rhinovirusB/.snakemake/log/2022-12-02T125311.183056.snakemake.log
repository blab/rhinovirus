Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 8
Rules claiming more threads will be scaled down.
Job stats:
job       count    min threads    max threads
------  -------  -------------  -------------
align         1              1              1
all           1              1              1
filter        1              1              1
parse         1              1              1
total         4              1              1

Select jobs to execute...

[Fri Dec  2 12:53:11 2022]
Job 3: Parsing fasta into sequences and metadata

[Fri Dec  2 12:53:12 2022]
Finished job 3.
1 of 4 steps (25%) done
Select jobs to execute...

[Fri Dec  2 12:53:12 2022]
Job 2: 
        Filtering to
          - 1000 sequence(s) per country
          - minimum genome length of 500
        

[Fri Dec  2 12:53:13 2022]
Finished job 2.
2 of 4 steps (50%) done
Select jobs to execute...

[Fri Dec  2 12:53:13 2022]
Job 1: 
        Aligning sequences to config/reference_rhinovirusB_all.gb
          - filling gaps with N
        

[Fri Dec  2 13:01:41 2022]
Finished job 1.
3 of 4 steps (75%) done
Select jobs to execute...

[Fri Dec  2 13:01:41 2022]
localrule all:
    input: results/aligned_rhinovirusB_partial.fasta
    jobid: 0
    resources: tmpdir=/var/folders/g_/6938g_6s199gxxswt2nf7w500000gn/T

[Fri Dec  2 13:01:41 2022]
Finished job 0.
4 of 4 steps (100%) done
Complete log: .snakemake/log/2022-12-02T125311.183056.snakemake.log
